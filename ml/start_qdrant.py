#!/usr/bin/env python
# coding: utf-8
"""
FAQ‚Äë–±–æ—Ç ¬´–ü—è—Ç—ë—Ä–æ—á–∫–∞¬ª (v3.3, 2025‚Äë05‚Äë05)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–ò–∑–º–µ–Ω–µ–Ω–∏—è –∫ v3.2 (Hybrid‚Äëready)
‚Ä¢ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞—ë—Ç—Å—è —Å –¥–≤—É–º—è –ò–ú–ï–ù–û–í–ê–ù–ù–´–ú–ò –≤–µ–∫—Ç–æ—Ä–∞–º–∏: dense¬†–∏ sparse.
‚Ä¢ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å fastembed (BM25 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ sparse‚Äë–≤–µ–∫—Ç–æ—Ä–æ–≤.
‚Ä¢ –ü—Ä–∏ upsert —Å–æ—Ö—Ä–∞–Ω—è–µ–º dense¬†+ sparse –≤–µ–∫—Ç–æ—Ä–∞ –æ–¥–Ω–æ–π —Ç–æ—á–∫–æ–π.
‚Ä¢ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Query API (prefetch + RRF).
‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ —á–∏—Å—Ç—ã–π dense‚Äë–ø–æ–∏—Å–∫, –µ—Å–ª–∏ fastembed –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
‚Ä¢ –û–±–Ω–æ–≤–ª–µ–Ω—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å¬†qdrant‚Äëclient¬†‚â•‚ÄØ1.7.
"""

from __future__ import annotations

import inspect
import logging
import os
import re
import sys
import warnings
from collections import Counter, namedtuple
from pathlib import Path
from typing import List, Tuple, Dict, Any

import torch
import pandas as pd
from qdrant_client import QdrantClient, models
from qdrant_client.http.exceptions import UnexpectedResponse
from sentence_transformers import SentenceTransformer, CrossEncoder
from transformers import AutoTokenizer, pipeline
from pymorphy2 import MorphAnalyzer
from stop_words import get_stop_words

# ‚Äî‚Äî‚Äî –ø–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–≥—Ä—É–∑–∏—Ç—å fastembed –¥–ª—è sparse‚Äë–≤–µ–∫—Ç–æ—Ä–æ–≤
try:
    from fastembed import SparseTextEmbedding, SparseEmbedding

    _HAS_FASTEMBED = True
except ImportError:  # –ø–∞–∫–µ—Ç –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    _HAS_FASTEMBED = False

# ‚Äî‚Äî‚Äî Monkey‚Äëpatch –¥–ª—è Py3.11+ (sentence‚Äëtransformers –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ deprecated¬†API)
if not hasattr(inspect, "getargspec"):
    ArgSpec = namedtuple("ArgSpec", "args varargs keywords defaults")

    def getargspec(func):  # type: ignore[misc]
        f = inspect.getfullargspec(func)
        return ArgSpec(f.args, f.varargs, f.varkw, f.defaults)

    inspect.getargspec = getargspec  # type: ignore[assignment]

# ‚Äî‚Äî‚Äî –∫–æ–Ω—Ñ–∏–≥
CSV = "faq_canonical_expanded.csv"
COLL = "x5_faq_prod_customer_v3"

EMB_NAME = "d0rj/e5-large-en-ru"
Q_PREF, P_PREF = "query: ", "passage: "

CE_NAME = "BAAI/bge-reranker-large"
CE_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CE_TOP_K = 20
TOP_N_AFTER_CE = 3

LLM_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

FIRST_THRESHOLD, SECOND_THRESHOLD = 0.72, 0.60
CE_ACCEPT, CE_FLOOR = 0.48, 0.30
NOUN_OVERLAP = 0.45
MAX_DIRECT_WORDS = 250
LOG_LVL = logging.INFO

REFUSAL = (
    "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à—ë–ª —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. "
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ ¬´–ü—è—Ç—ë—Ä–æ—á–∫–∏¬ª."
)

# ‚Äî‚Äî‚Äî –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=LOG_LVL,
    format="%(asctime)s | %(levelname)-8s | %(name)s:%(lineno)d | %(message)s",
    stream=sys.stdout,
)
for noisy in (
    "httpx",
    "httpcore",
    "sentence_transformers",
    "transformers.generation_utils",
):
    logging.getLogger(noisy).setLevel(logging.WARNING)
warnings.filterwarnings("ignore", category=DeprecationWarning)
log = logging.getLogger("FAQ-Bot")

# ‚Äî‚Äî‚Äî NLP‚Äë—É—Ç–∏–ª–∏—Ç—ã
morph = MorphAnalyzer()
_word_re = re.compile(r"[–∞-—è—ëa-z]+", re.I)
STOP_RU = set(get_stop_words("russian"))


def lemmatize(text: str) -> List[Tuple[str, str]]:
    pairs: List[Tuple[str, str]] = []
    for w in _word_re.findall(text.lower()):
        if w in STOP_RU:
            continue
        p = morph.parse(w)[0]
        pairs.append((p.normal_form, p.tag.POS or ""))
    return pairs


def extract_nouns(pairs: List[Tuple[str, str]]) -> set[str]:
    return {lemma for lemma, pos in pairs if pos == "NOUN"}


COUNTRIES = r"(?:—Ä–æ—Å—Å–∏[–∏—è]|—Ä—Ñ|–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω|–∫–∑|–±–µ–ª–∞—Ä—É—Å[—å–∏]|—Ä–±|—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω|kg|–∫—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω)"
SPURIOUS_LOC_RE = re.compile(rf"\b(?:–≤|–≤–æ)\s+{COUNTRIES}\b", re.I)


class FAQBot:
    """–ß–∞—Ç‚Äë–±–æ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (dense¬†+ sparse)."""

    def __init__(self):
        log.info("üîÑ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞‚Ä¶")

        # ‚ë† dense‚Äë—ç–º–±–µ–¥–¥–µ—Ä
        self.emb = SentenceTransformer(EMB_NAME).to(CE_DEVICE)
        self.dim = self.emb.get_sentence_embedding_dimension()
        log.info("üìê embedder %s, dim=%d", EMB_NAME, self.dim)

        # ‚ë° cross‚Äëencoder –¥–ª—è rerank
        self.ce = CrossEncoder(CE_NAME, device=CE_DEVICE)
        log.info("üîó cross‚Äëencoder %s –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ %s", CE_NAME, CE_DEVICE)

        # ‚ë¢ sparse‚Äë—ç–Ω–∫–æ–¥–µ—Ä (optional)
        self._has_sparse = False
        if _HAS_FASTEMBED:
            try:
                # BM25 –Ω–µ —Ç—Ä–µ–±—É–µ—Ç GPU –∏ –≤–µ—Å–∏—Ç <10‚ÄØ–ú–ë
                self.sparse_enc = SparseTextEmbedding(model_name="Qdrant/bm25")
                self._has_sparse = True
                log.info("üß© sparse‚Äëencoder fastembed/BM25 –≥–æ—Ç–æ–≤")
            except Exception as e:
                log.warning("‚ùó –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å fastembed: %s", e)
                self._has_sparse = False
        else:
            log.info("‚ÑπÔ∏è fastembed –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω; —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")

        # ‚ë£ Qdrant
        self.qdr = QdrantClient(url="http://localhost:6333", timeout=90)
        self._ensure_collection()

        # ‚ë§ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫—Ä–∞—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
        self.tok = AutoTokenizer.from_pretrained(LLM_NAME, padding_side="left")
        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token
        self.llm = pipeline(
            "text-generation",
            model=LLM_NAME,
            tokenizer=self.tok,
            device_map="auto",
            do_sample=True,
            temperature=0.15,
            top_p=0.90,
            repetition_penalty=1.2,
            torch_dtype=(
                torch.bfloat16
                if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
                else torch.float32
            ),
        )
        log.info("‚úÖ –±–æ—Ç –≥–æ—Ç–æ–≤")

    # ‚Äî‚Äî‚Äî —Ä–∞–±–æ—Ç–∞ —Å –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π
    def _collection_exists(self) -> bool:
        return COLL in [c.name for c in self.qdr.get_collections().collections]

    def _ensure_collection(self):
        """–°–æ–∑–¥–∞—Ç—å (–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å) –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å dense¬†+ sparse –≤–µ–∫—Ç–æ—Ä–∞–º–∏."""
        if not self._collection_exists():
            log.info("üÜï —Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é %s (dense + sparse)‚Ä¶", COLL)
            self.qdr.create_collection(
                collection_name=COLL,
                vectors_config={
                    "dense": models.VectorParams(
                        size=self.dim, distance=models.Distance.COSINE
                    )
                },
                sparse_vectors_config={"sparse": models.SparseVectorParams()},
            )
        else:
            # –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è sparse‚Äë—Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            info = self.qdr.get_collection(COLL)
            if "sparse" not in info.sparse_vectors_config:
                log.warning(
                    "‚ùó –ö–æ–ª–ª–µ–∫—Ü–∏—è %s —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –±–µ–∑ sparse‚Äë–≤–µ–∫—Ç–æ—Ä–∞. "
                    "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç.",
                    COLL,
                )
                self._has_sparse = False

        # –∑–∞–≥—Ä—É–∑–∏–º CSV –∏ –¥–æ–ª—å—ë–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ç–æ—á–∫–∏
        df = (
            pd.read_csv(CSV, dtype=str)
            .dropna(subset=["id", "question", "content"])
            .loc[:, ["id", "question", "content"]]
        )

        points, _ = self.qdr.scroll(COLL, with_payload=False, limit=1_000_000)
        existing = {p.id for p in points}

        new = df.loc[~df["id"].astype(int).isin(existing)].reset_index(drop=True)
        if new.empty:
            log.info("üì¶ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞, –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –Ω–µ—Ç")
            return

        log.info("‚ÜóÔ∏è upsert %d –Ω–æ–≤—ã—Ö Q‚ÄëA", len(new))

        dense_texts = [f"{P_PREF}{row.question} {row.content[:512]}" for row in new.itertuples()]
        dense_vecs = self.emb.encode(dense_texts, convert_to_tensor=False, show_progress_bar=False)

        sparse_embs: List[Tuple[List[int], List[float]]] = []
        if self._has_sparse:
            for emb in self.sparse_enc.embed_documents([row.question for row in new.itertuples()]):
                sparse_embs.append((emb.indices.tolist(), emb.values.tolist()))
        else:
            sparse_embs = [([], [])] * len(new)

        up_points: List[models.PointStruct] = []
        for i, row in enumerate(new.itertuples()):
            vectors: Dict[str, Any] = {"dense": dense_vecs[i].tolist()}
            if self._has_sparse:
                idx, val = sparse_embs[i]
                vectors["sparse"] = models.SparseVector(indices=idx, values=val)
            up_points.append(
                models.PointStruct(
                    id=int(row.id),
                    vector=vectors,  # type: ignore[arg-type]
                    payload={"question": row.question, "answer": row.content},
                )
            )

        for i in range(0, len(up_points), 128):
            self.qdr.upsert(COLL, up_points[i : i + 128], wait=True)

        total = self.qdr.count(COLL, exact=True).count
        log.info("üì¶ –≤—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: %d", total)

    # ‚Äî‚Äî‚Äî utils
    @staticmethod
    def _clean_query(q: str) -> str:
        q = SPURIOUS_LOC_RE.sub("", q)
        q = re.sub(r"\s{2,}", " ", q)
        return q.strip()

    def _adaptive_threshold(self, vec: List[float]) -> float:
        base = self.qdr.search(COLL, query_vector=("dense", vec), limit=20)
        if not base:
            return FIRST_THRESHOLD
        scores = sorted(h.score for h in base)
        idx = int(len(scores) * 0.35)
        return float(max(SECOND_THRESHOLD, scores[idx]))

    def _passes_noun_gate(self, q_pairs: List[Tuple[str, str]], txt: str, ce_score: float) -> bool:
        if ce_score < CE_ACCEPT:
            return False
        qn = extract_nouns(q_pairs)
        hn = extract_nouns(lemmatize(txt))
        if not qn:
            return ce_score >= CE_FLOOR
        overlap = len(qn & hn) / len(qn)
        return overlap >= NOUN_OVERLAP

    def _best_paragraph(self, q_pairs: List[Tuple[str, str]], answer: str) -> str:
        paras = [p.strip() for p in re.split(r"\n{2,}|\r?\n", answer) if p.strip()]
        if len(paras) == 1:
            return paras[0]
        qn = extract_nouns(q_pairs)
        best, best_ol = paras[0], 0.0
        for p in paras:
            ol = len(qn & extract_nouns(lemmatize(p))) / max(1, len(qn))
            if ol > best_ol:
                best, best_ol = p, ol
        return best

    # ‚Äî‚Äî‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥
    def ask(self, raw_query: str) -> str:
        raw_query = raw_query.strip()
        if not raw_query:
            return REFUSAL

        log.info("üí¨ –≤–æ–ø—Ä–æ—Å: %s", raw_query)
        query = self._clean_query(raw_query)
        q_pairs = lemmatize(query)
        q_vec = self.emb.encode(Q_PREF + query, convert_to_tensor=False).tolist()

        # ‚ë† —Ñ–æ—Ä–º–∏—Ä—É–µ–º sparse‚Äë–≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        sparse_q = None
        if self._has_sparse:
            emb = self.sparse_enc.embed_query(query)
            sparse_q = models.SparseVector(indices=emb.indices.tolist(), values=emb.values.tolist())

        # ‚ë° –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        hits = []
        try:
            if self._has_sparse and sparse_q and len(sparse_q.indices) > 0:
                # –≥–∏–±—Ä–∏–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Query API
                hits = self.qdr.query_points(
                    collection_name=COLL,
                    prefetch=[
                        models.Prefetch(query=sparse_q, using="sparse", limit=50),
                        models.Prefetch(query=q_vec, using="dense", limit=50),
                    ],
                    query=models.FusionQuery(fusion=models.Fusion.RRF),
                    limit=CE_TOP_K,
                )
            else:
                # fallback ‚Üí —á–∏—Å—Ç—ã–π dense
                hits = self.qdr.search(
                    COLL,
                    query_vector=("dense", q_vec),
                    limit=CE_TOP_K,
                    score_threshold=self._adaptive_threshold(q_vec),
                )
        except UnexpectedResponse as e:
            log.warning("–û—Ç–≤–µ—Ç Qdrant: %s ‚Äî fallback –Ω–∞ dense", e)
            hits = self.qdr.search(
                COLL,
                query_vector=("dense", q_vec),
                limit=CE_TOP_K,
                score_threshold=self._adaptive_threshold(q_vec),
            )

        if not hits:
            return REFUSAL

        # ‚ë¢ rerank
        ce_pairs = [(query, f"{h.payload['question']} {h.payload['answer']}") for h in hits]
        ce_scores = self.ce.predict(ce_pairs, convert_to_numpy=True, batch_size=16)
        scored = sorted(zip(ce_scores, hits), key=lambda x: x[0], reverse=True)[: TOP_N_AFTER_CE]

        answer: str | None = None
        for best_s, best_hit in scored:
            doc_text = f"{best_hit.payload['question']} {best_hit.payload['answer']}"
            if self._passes_noun_gate(q_pairs, doc_text, best_s):
                answer = best_hit.payload["answer"].strip()
                break
        if answer is None:
            return REFUSAL

        # ‚ë£ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        if len(answer.split()) <= MAX_DIRECT_WORDS:
            return answer

        snippet = self._best_paragraph(q_pairs, answer)
        if len(snippet.split()) <= MAX_DIRECT_WORDS:
            return snippet

        context = snippet[:1024]
        msgs = [
            {
                "role": "system",
                "content": "–¢—ã ‚Äî –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ —Å–µ—Ç–∏ ¬´–ü—è—Ç—ë—Ä–æ—á–∫–∞¬ª. "
                "–°–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–π—Å—è –Ω–∞ —Ç–æ—á–Ω–æ–º –∏ –∫—Ä–∞—Ç–∫–æ–º –æ—Ç–≤–µ—Ç–µ.",
            },
            {"role": "system", "content": f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}"},
            {"role": "user", "content": raw_query},
        ]
        prompt = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        out = self.llm(
            prompt,
            max_new_tokens=120,
            stop=[self.tok.eos_token],
            pad_token_id=self.tok.pad_token_id,
            no_repeat_ngram_size=3,
            return_full_text=False,
        )[0]["generated_text"].strip()
        return out or snippet or REFUSAL

    def close(self):
        self.qdr.close()


# ‚Äî‚Äî‚Äî –º–∏–Ω–∏‚Äë—Ç–µ—Å—Ç
if __name__ == "__main__":
    bot = FAQBot()
    while True:
        test_q = input()
        print("‚Äî" * 100)
        print("–í–æ–ø—Ä–æ—Å:", test_q)
        print("–û—Ç–≤–µ—Ç :", bot.ask(test_q))
    bot.close()
