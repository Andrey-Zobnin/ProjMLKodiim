#!/usr/bin/env python
# coding: utf‚Äë8
"""
FAQ‚Äë–±–æ—Ç ¬´–ü—è—Ç—ë—Ä–æ—á–∫–∞¬ª (v2, 2025‚Äë05‚Äë05)
–ò–∑–º–µ–Ω–µ–Ω–∏—è:
‚Ä¢ –º—è–≥—á–µ –ø–æ—Ä–æ–≥–∏¬†(FIRST/SECOND, CE_ACCEPT/CE_FLOOR)
‚Ä¢ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –æ—Ç 35‚Äë–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è
‚Ä¢ –æ—á–∏—Å—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç –≥–µ–æ—Ç–µ–≥–æ–≤ (–†–æ—Å—Å–∏—è/–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω/‚Ä¶)
‚Ä¢ —É–¥–∞–ª–µ–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø‚Äë—Å–ª–æ–≤ –ø—Ä–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
‚Ä¢ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ TOP_K/CE_TOP_K
‚Ä¢ do_sample=True, stop‚Äë—Ç–æ–∫–µ–Ω –¥–ª—è TinyLlama
"""

import inspect
import os
import sys
import logging
import warnings
import re
from collections import namedtuple
from pathlib import Path

if not hasattr(inspect, "getargspec"):
    # –ø–∞—Ç—á –¥–ª—è Py3.11+
    ArgSpec = namedtuple("ArgSpec", "args varargs keywords defaults")
    def getargspec(func):
        f = inspect.getfullargspec(func)
        return ArgSpec(f.args, f.varargs, f.varkw, f.defaults)
    inspect.getargspec = getargspec

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –±–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import torch
import pandas as pd
from sentence_transformers import SentenceTransformer, CrossEncoder
from transformers import pipeline, AutoTokenizer
from qdrant_client import QdrantClient, models
from pymorphy2 import MorphAnalyzer
from stop_words import get_stop_words          # ‚Üê —Å—Ç–æ–ø‚Äë—Å–ª–æ–≤–∞

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –∫–æ–Ω—Ñ–∏–≥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CSV      = "faq_canonical_expanded.csv"
COLL     = "x5_faq_prod_customer_v2"

EMB_NAME   = "d0rj/e5-large-en-ru"
Q_PREF, P_PREF = "query: ", "passage: "

CE_NAME    = "DiTy/cross-encoder-russian-msmarco"
CE_DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"
CE_TOP_K   = 20                               # –±—ã–ª–æ 10

LLM_NAME   = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

FIRST_THRESHOLD, SECOND_THRESHOLD = 0.72, 0.60  # –±—ã–ª–æ 0.78 / 0.68
FALLBACK_TOPK, TOP_K             = 25, 25        # —à–∏—Ä–µ –æ–∫–Ω–æ
CE_ACCEPT, CE_FLOOR              = 0.45, 0.28    # –±—ã–ª–æ 0.48 / 0.32
LEMMA_MIN_OVERLAP                = 0.35
MAX_DIRECT_WORDS                 = 250           # –¥–ª–∏–Ω–Ω–µ–µ –ª–∏–º–∏—Ç
LOG_LVL                          = logging.INFO

REFUSAL = (
    "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à—ë–ª —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. "
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ ¬´–ü—è—Ç—ë—Ä–æ—á–∫–∏¬ª."
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=LOG_LVL,
    format="%(asctime)s | %(levelname)-8s | %(name)s:%(lineno)d | %(message)s",
    stream=sys.stdout,
)
for noisy in (
    "httpx", "httpcore",
    "sentence_transformers",
    "transformers.generation_utils"
):
    logging.getLogger(noisy).setLevel(logging.WARNING)
warnings.filterwarnings("ignore", category=DeprecationWarning)
log = logging.getLogger("FAQ-Bot")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ —É—Ç–∏–ª–∏—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
morph = MorphAnalyzer()
_word_re = re.compile(r"[–∞-—è—ëa-z]+", re.I)

STOP_RU = set(get_stop_words("russian"))

def lemmatize(text: str) -> set[str]:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è + —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø‚Äë—Å–ª–æ–≤."""
    return {
        morph.parse(w)[0].normal_form
        for w in _word_re.findall(text.lower())
        if w not in STOP_RU
    }

COUNTRIES = r"(?:—Ä–æ—Å—Å–∏[–∏—è]|—Ä—Ñ|–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω|–∫–∑|–±–µ–ª–∞—Ä—É—Å[—å–∏]|—Ä–±|—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω|kg|–∫—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω)"
SPURIOUS_LOC_RE = re.compile(rf"\b(?:–≤|–≤–æ)\s+{COUNTRIES}\b", re.I)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –±–æ—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class FAQBot:
    def __init__(self):
        log.info("üîÑ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞‚Ä¶")

        # —ç–º–±–µ–¥–¥–µ—Ä
        self.emb = SentenceTransformer(EMB_NAME).to(CE_DEVICE)
        self.dim = self.emb.get_sentence_embedding_dimension()
        log.info("üìê embedder %s, dim=%d", EMB_NAME, self.dim)

        # reranker
        self.ce = CrossEncoder(CE_NAME, device=CE_DEVICE)
        log.info("üîó cross-encoder %s –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ %s", CE_NAME, CE_DEVICE)

        # –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î
        self.qdr = QdrantClient("localhost", port=6333, timeout=90)
        self._ensure_collection()

        # LLM
        self.tok = AutoTokenizer.from_pretrained(LLM_NAME, padding_side="left")
        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token
        self.llm = pipeline(
            "text-generation",
            model=LLM_NAME,
            tokenizer=self.tok,
            device_map="auto",
            do_sample=True,              # –≤–∫–ª—é—á–∞–µ–º —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            temperature=0.15,
            top_p=0.90,
            repetition_penalty=1.2,
            torch_dtype=(
                torch.bfloat16
                if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
                else torch.float32
            ),
        )
        log.info("‚úÖ –±–æ—Ç –≥–æ—Ç–æ–≤")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ —Ä–∞–±–æ—Ç–∞ —Å –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _collection_exists(self) -> bool:
        return COLL in [c.name for c in self.qdr.get_collections().collections]

    def _ensure_collection(self):
        if not self._collection_exists():
            self.qdr.create_collection(
                COLL,
                vectors_config=models.VectorParams(
                    size=self.dim,
                    distance=models.Distance.COSINE
                )
            )
            log.info("üÜï —Å–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è %s", COLL)

        df = (
            pd.read_csv(CSV, dtype=str)
              .dropna(subset=["id", "question", "content"])
              .loc[:, ["id", "question", "content"]]
        )

        points, _ = self.qdr.scroll(COLL, with_payload=False, limit=1_000_000)
        existing = {p.id for p in points}

        new = df.loc[~df["id"].astype(int).isin(existing)].reset_index(drop=True)
        if new.empty:
            log.info("üì¶ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞, –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –Ω–µ—Ç")
            return

        log.info("‚ÜóÔ∏è upsert %d –Ω–æ–≤—ã—Ö Q‚ÄëA", len(new))
        texts = [
            f"{P_PREF}{row.question} {row.content[:512]}"
            for row in new.itertuples()
        ]
        vecs = self.emb.encode(texts, convert_to_tensor=False, show_progress_bar=False)
        points = [
            models.PointStruct(
                id=int(row.id),
                vector=vecs[i].tolist(),
                payload={"question": row.question, "answer": row.content}
            )
            for i, row in enumerate(new.itertuples())
        ]
        for i in range(0, len(points), 128):
            self.qdr.upsert(COLL, points[i:i+128], wait=True)

        total = self.qdr.count(COLL, exact=True).count
        log.info("üì¶ –∫–æ–ª–ª–µ–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞, –≤—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: %d", total)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _clean_query(self, q: str) -> str:
        """–£–¥–∞–ª—è–µ–º —É–∫–∞–∑–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã/—Ä–µ–≥–∏–æ–Ω–∞, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã."""
        q = SPURIOUS_LOC_RE.sub("", q)
        q = re.sub(r"\s{2,}", " ", q)
        return q.strip()

    def _adaptive_threshold(self, vec: list[float]) -> float:
        """–°—á–∏—Ç–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ = 35‚Äë–ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –∏–∑ —Ç–æ–ø‚Äë20."""
        base = self.qdr.search(COLL, vec, limit=20)
        if not base:
            return FIRST_THRESHOLD
        scores = sorted(h.score for h in base)
        idx = int(len(scores) * 0.35)
        return max(SECOND_THRESHOLD, scores[idx])

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def ask(self, raw_query: str) -> str:
        raw_query = raw_query.strip()
        if not raw_query:
            return REFUSAL

        log.info("üí¨ –≤–æ–ø—Ä–æ—Å: %s", raw_query)

        query = self._clean_query(raw_query)
        q_vec = self.emb.encode(Q_PREF + query, convert_to_tensor=False).tolist()

        dyn_thr = self._adaptive_threshold(q_vec)
        hits = (
            self.qdr.search(COLL, q_vec, limit=TOP_K, score_threshold=dyn_thr)
            or self.qdr.search(COLL, q_vec, limit=TOP_K, score_threshold=SECOND_THRESHOLD)
            or self.qdr.search(COLL, q_vec, limit=FALLBACK_TOPK)
        )
        if not hits:
            return REFUSAL

        # rerank
        ce_pairs = [
            (query, f"{h.payload['question']} {h.payload['answer']}")  # noqa: E501
            for h in hits[:CE_TOP_K]
        ]
        ce_scores = self.ce.predict(ce_pairs, convert_to_numpy=True, batch_size=16)
        best_s, best_hit = max(zip(ce_scores, hits[:CE_TOP_K]), key=lambda x: x[0])
        log.debug("üè∑ id=%s cos=%.3f ce=%.3f", best_hit.id, best_hit.score, best_s)

        # —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
        if best_s < CE_ACCEPT:
            ql = lemmatize(query)
            hl = lemmatize(
                best_hit.payload["question"] + " " + best_hit.payload["answer"]
            )
            overlap = len(ql & hl) / max(1, len(ql))
            if best_s < CE_FLOOR or overlap < LEMMA_MIN_OVERLAP:
                return REFUSAL

        answer = best_hit.payload["answer"].strip()
        if len(answer.split()) <= MAX_DIRECT_WORDS:
            return answer

        # –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ‚Äî —Ä–µ–∑—é–º–∏—Ä—É–µ–º LLM –Ω–∞ –ø–µ—Ä–≤—ã—Ö 1‚ÄØ024 —Å–∏–º–≤–æ–ª–∞—Ö
        context = answer[:1024]
        msgs = [
            {"role": "system", "content": "–¢—ã ‚Äî –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ —Å–µ—Ç–∏ ¬´–ü—è—Ç—ë—Ä–æ—á–∫–∞¬ª."},
            {"role": "system", "content": f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}"},
            {"role": "user",   "content": raw_query}
        ]
        prompt = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        out = self.llm(
            prompt,
            max_new_tokens=120,
            stop=[self.tok.eos_token],
            pad_token_id=self.tok.pad_token_id,
            no_repeat_ngram_size=3,
            return_full_text=False
        )[0]["generated_text"].strip()

        return out or REFUSAL

    def close(self):
        self.qdr.close()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ —Ç–µ—Å—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    bot = FAQBot()
    tests = [
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∏ –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω?",
        "–º–Ω–µ –Ω—É–∂–Ω–æ –æ—Ñ–æ—Ä–º–∏—Ç—å –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫—É, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å?",
        "–ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ, –≤ –∫–∞–∫–∏–µ —Å—Ä–æ–∫–∏ –ø–µ—Ä–µ—á–∏—Å–ª—è—Ç –≤—ã–ø–ª–∞—Ç—É –∑–∞ –º–æ–π –µ–∂–µ–≥–æ–¥–Ω—ã–π –æ—Ç–ø—É—Å–∫?"
    ]
    for q in tests:
        print("‚Äî" * 100)
        print("–í–æ–ø—Ä–æ—Å:", q)
        print("–û—Ç–≤–µ—Ç :", bot.ask(q))
    bot.close()
